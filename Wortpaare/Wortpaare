# Sicherstellen, dass alle benötigten Pakete installiert sind
if (!require(tm)) install.packages("tm")
if (!require(slam)) install.packages("slam")
if (!require(widyr)) install.packages("widyr")
if (!require(ggraph)) install.packages("ggraph")
if (!require(igraph)) install.packages("igraph")
if (!require(dplyr)) install.packages("dplyr")
if (!require(tidytext)) install.packages("tidytext")
if (!require(stringr)) install.packages("stringr")
if (!require(SnowballC)) install.packages("SnowballC")

library(tm)
library(slam)
library(widyr)
library(ggraph)
library(igraph)
library(dplyr)
library(tidytext)
library(stringr)
library(SnowballC)

# Den Korpus laden (Pfad anpassen, falls nötig)
data <- read.csv("Medien_Gesamt_bereinigt.csv", stringsAsFactors = FALSE)

# Überprüfen, ob die Spalte "cleaned_hits" vorhanden ist
if (!"cleaned_hits" %in% colnames(data)) {
  stop("Die Spalte 'cleaned_hits' ist nicht im Datensatz enthalten!")
}

# Nur die Spalte "cleaned_hits" verwenden
cleaned_hits <- data$cleaned_hits

# Ersetzungsregeln definieren
replacements <- c(
  "staates" = "staat",
  "ministerpräsidenten" = "ministerpräsident",
  "staaten" = "staat",
  "nahen" = "nah",
  "osten" = "ost",
  "prasidenten" = "prasident",
  "militante" = "militant",
  "verhandlungen" = "verhandlung",
  "besetzten" = "besetzt",
  "gebiete" = "gebiet",
  "gebieten" = "gebiet", 
  "gebietn" = "gebiet",
  "arabische" = "arabisch", 
  "arabischn" = "arabisch",
  "arabischen" = "arabisch",
  "konflikte" = "konflikt",
  "konfliktes" = "konflikt",
  "regierungen" = "regierung",
  "fluchtlinge" = "fluchtling"
)

# Anwenden der Ersetzungen auf jeden Text in cleaned_hits
for (pattern in names(replacements)) {
  replacement <- replacements[[pattern]]
  # ignore.case=TRUE kann hinzugefügt werden, falls Groß-/Kleinschreibung ignoriert werden soll
  cleaned_hits <- str_replace_all(cleaned_hits, pattern, replacement)
}

# Definiere die Liste der Stopwörter, einschließlich der hinzugefügten Wörter
custom_stopwords <- c("ab", "wegen", "ersten", "dabei", "sowie")

# Tokenisieren der "cleaned_hits"-Spalte in einzelne Wörter
hits_words <- tibble(id = 1:length(cleaned_hits), text = cleaned_hits) %>%
  unnest_tokens(word, text)  # Tokenisieren in einzelne Wörter

# Anwenden des Stemmings auf die Tokens
hits_words <- hits_words %>%
  mutate(word = wordStem(word, language = "german"))

# Entferne Stopwörter
hits_words_clean <- hits_words %>%
  filter(!word %in% custom_stopwords)

# Wortpaare aus der "cleaned_hits"-Spalte berechnen
word_pairs <- hits_words_clean %>% 
  pairwise_count(word, id, sort = TRUE, upper = FALSE)  # Wortpaare berechnen

# Wortpaare mit einer Mindestanzahl von 2700 (z.B. nur häufige Paare)
word_pairs_filtered <- word_pairs %>% 
  filter(n >= 2700)  # Filtert nur Paare mit mindestens 2700 Vorkommen
