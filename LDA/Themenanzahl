# Notwendige Pakete laden
library(topicmodels)
library(slam)
library(broom)
library(tidytext)
library(tm)
library(SnowballC)

# === 1. Datei einlesen ===
data <- read.csv("/Users/cedricgold/Downloads/Medien_Gesamt_bereinigt.csv", stringsAsFactors = FALSE)

# Prüfen, ob die erwartete Spalte existiert
if (!"cleaned_hits" %in% colnames(data)) {
  stop("Fehler: Die Spalte 'cleaned_hits' wurde nicht gefunden. Überprüfe die Spaltennamen mit colnames(data).")
}

# === 2. Nur die relevante Spalte nutzen ===
text_data <- data$cleaned_hits

# Fehlende Werte entfernen
text_data <- text_data[!is.na(text_data)]

# === 3. Benutzerdefinierte Stoppwörter definieren ===
custom_stopwords <- c("erst", "geboren", "berlin", "muss", "streif", "tag", "offent", "jassi")

# === 4. Textvorverarbeitung ===
docs <- Corpus(VectorSource(text_data))
docs <- tm_map(docs, content_transformer(tolower))  # Kleinbuchstaben
docs <- tm_map(docs, removePunctuation)  # Satzzeichen entfernen
docs <- tm_map(docs, removeNumbers)  # Zahlen entfernen
docs <- tm_map(docs, removeWords, stopwords("de"))  # Deutsche Stoppwörter entfernen
docs <- tm_map(docs, removeWords, custom_stopwords)  # Benutzerdefinierte Stoppwörter entfernen
docs <- tm_map(docs, stemDocument, language = "german")  # Stemming

# === 5. Dokument-Term-Matrix (DTM) erstellen ===
dtm <- DocumentTermMatrix(docs)
dtm <- as.matrix(dtm)  # Umwandeln für weitere Verarbeitung

# === 6. Seltene Begriffe entfernen ===
row_totals <- slam::row_sums(dtm)
dtm_filtered <- dtm[row_totals > 0, ]

if (nrow(dtm_filtered) == 0) {
  stop("Fehler: Nach der Filterung enthält die DTM keine Dokumente mehr. Prüfe die Vorverarbeitung!")
}

# === 7. Optimale Anzahl an Themen bestimmen (ab k = 2!) ===
topic_results <- data.frame(
  k = integer(),
  Perplexity = numeric(),
  LogLikelihood = numeric()
)

for (k in 2:15) {  # Startet jetzt bei 2!
  lda_model <- LDA(dtm_filtered, k = k, method = "VEM", control = list(seed = 1234, alpha = 0.1))
 
  # Perplexität berechnen
  perplexity_value <- perplexity(lda_model, dtm_filtered)
 
  # Log-Likelihood berechnen
  log_likelihood <- as.numeric(logLik(lda_model))
 
  # Ergebnisse speichern
  topic_results <- rbind(topic_results, data.frame(k = k, Perplexity = perplexity_value, LogLikelihood = log_likelihood))
 
  cat(sprintf("Themen: %d | Perplexität: %.2f | Log-Likelihood: %.2f\n", k, perplexity_value, log_likelihood))
}

# Optimale Anzahl an Themen bestimmen (Minimale Perplexität)
optimal_k <- topic_results$k[which.min(topic_results$Perplexity)]
cat(sprintf("\nOptimale Anzahl an Themen: %d\n", optimal_k))

# === 8. Ergebnisse als CSV speichern ===
output_path <- "/Users/cedricgold/Downloads/Themenanalyse_Ergebnisse.csv"
write.csv(topic_results, output_path, row.names = FALSE)

cat(sprintf("\nErgebnisse wurden gespeichert unter: %s\n", output_path))

# Ergebnisse anzeigen
print(topic_results)
