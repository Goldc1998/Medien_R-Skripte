# Pakete laden 
library(dplyr)
library(stringr)
library(tidyr)
library(readr)
library(data.table)
library(ggplot2)
library(tidytext)
library(widyr)

# Beispiel-Daten einlesen (ersetze dies mit deinem eigenen Datensatz)
medien_combined_cleaned <- read_csv("Medien_Gesamt_bereinigt.csv")



# Text bereinigen und in Tokens umwandeln
cleaned_hits_df <- medien_combined_cleaned %>% 
  select(NewID, cleaned_hits) %>%  # Wähle die relevanten Spalten aus
  unnest_tokens(word, cleaned_hits) %>%  # Diese Funktion gehört zum 'tidytext' Paket
  mutate(word = tolower(word))  # Alle Wörter in Kleinbuchstaben umwandeln

# Vereinheitlichung der Wörter, basierend auf der Ersatz-Liste
cleaned_hits_df <- cleaned_hits_df %>% 
  mutate(word = recode(word, !!!replacement_words))

# Berechnung der Korrelationen
word_cors <- cleaned_hits_df %>% 
  group_by(word) %>% 
  filter(n() >= 500) %>%  # Hier filtern wir Wörter, die mindestens 500 Mal vorkommen
  pairwise_cor(word, NewID, sort = TRUE)

# Visualisierungen für die genannten Wörter (Top 20 Korrelationen):

# Liste der Begriffe
terms <- c("fatah", "angriff", "netanjahu", "siedlung", "abbas", "israel", "likud", "knesset", "verteidigung")

# Schleife über alle Begriffe und Visualisierung erstellen
for (term in terms) {
  # Visualisierung für jedes Wort
  plot <- word_cors %>% 
    filter(item1 == term) %>%  # Nur den aktuellen Begriff wählen
    slice_max(correlation, n = 20) %>%  # Die Top 20 Korrelationen auswählen
    ungroup() %>% 
    mutate(item2 = reorder(item2, correlation)) %>% 
    ggplot(aes(item2, correlation)) +
    geom_bar(stat = "identity") +
    coord_flip() +
    labs(x = "Korrelation", y = "Wort", title = paste("Wörter mit höchster Korrelation (", toupper(term), ")", sep = ""))

  # Speichern der Visualisierung für das aktuelle Wort als PNG-Datei
  ggsave(paste("wort_korrelationen_top20_", term, ".png", sep = ""), plot = plot, width = 10, height = 6)
}
